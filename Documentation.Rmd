---
title: "Documentation"
author: "JW"
date: "10 October 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem description

The goal of the capstone project is to use what we have learned so far in the data science spezialization course to built predictive models for natural language processing.Natural language processing is the task to program computers to handle large amounts of natural language data for further analysis.

For this we will work with a dataset from SwiftKey (https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). In this collection, there are 3 types of text sources (text from blogs, news and twitter) seperated by the main language used in the text.

For the further analysis we are going to analyze the *.blogs.txt files. The goal is to predict the next word one is going to type when one has already started typing a sentence. For this task we use the given natural language data to analyse words which are commonly used together. 

Throughout the project we will use the process learned so far:  
1. Define the question  
2. Define the ideal data set  
3. Determine what data you can access  
4. Obtain the data  
5. Clean the data  
6. Exploratory data analysis  
7. Statistical prediction/clustering  
8. Interpret results  
9. Challenge results  
10. Synthezise/write up results  
11. Create reproducible code  
12. Distribute results to other people  

Thoughout this project the tidytext package, nicely described in the book tidy text mining (see references), is used.

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(VGAM)
data("stop_words")
```

## Definining the question and the ideal data set

This is a predictive analysis problem. The question we want to answer is:
Given a collection of words, like the beginning of a sentence, what is/are the ( most probable ) word/s that come/s next?

To answer this question one could think about the following data to use:  
* a collection of natural language texts to analyze 
* a list of abbreviations  
* a list of synonyms  
* a list of buzzwords which are "up-to-date" (e.g. on a specific date there was a special event everyone is talking about)  

As we will not analyze here with a time dependency we can easily ignore the up-to-date buzzwords list.
The collection of natural language texts is given by swiftkey.

## Researching accessible data

## Obtaining the data 

The main data set we are going to use is the one given by SwiftKey. 
As the documents are quite big and we handle a predictive analysis problem, we sample our data into training, test and validation set.

```{r}
textLines<-readLines("en_US.blogs.txt", encoding="UTF-8")
trainingSet<-sample(textLines, length(textLines)*0.1)
testSet<-sample(setdiff(textLines, trainingSet), length(textLines)*0.9)
validationSet<-setdiff(setdiff(textLines, trainingSet), testSet)

# not used except for answering quiz questions
#textLinesTwitter<-readLines("en_US.twitter.txt", encoding="UTF-8") #2360148
#textLinesNews<-readLines("en_US.news.txt", encoding="UTF-8") #SUBs were manually removed via Notepad##
```

Furtherout we clean and analyze the trainingSet to build our predictive model with.

## Cleaning the data

Tidy data has by definition the following attributes:  

- each variable is in one column  
- each observation is in one row  
- there is one table for each "kind" of variable  
- there is a row with the column names  
- variable names should be human readable  

For natural language this is no other. The tidy text format is a table with "one-token-by-row". Therefore we have to so called tokezine our data. A token is a meaningful unit of text, such as a word. Tokenizing means splitting the text data into tokens.

Furthermore one could think about transforming the text data further with the following tasks:

-  stemming (see Porter's algorithm)  
-  whitespace removal  
-  stopword removal  
-  punctuation removal
-  lower case conversion (might remove some knowledge)  

With the tidytext package this is easily done. The unnest_tokens function does most of the work. The function tidies the data regarding punctuation removal and lower case conversion. Furthermore it includes a tokenization. The default tokenizing in the unnest_functions is for words, but other options include characters, n-grams, sentences, lines, paragraphs, or separation around a regex pattern. 
The stopword removal can be done on basis of the dataset $stopwords$.
At this point we prepare data sets for words as tokens, 2-,3-,4- and 5-grams with and without stopwords for further analysis.

There is a code book provided which describes the variables in the tidy data set.
This documents shall be the instruction list showing the transformations made.

```{r}
footer<-grep(".+ is a participant in the Amazon Services LLC and Amazon EU Associates Programmes designed to provide a means for sites to earn advertising fees by advertising and linking to amazon.com, amazon.ca, amazon.co.uk, amazon.de, amazon.fr, amazon.it and amazon.es. Certain content that appears on this website comes from Amazon Services LLC and/or Amazon EU. This content is provided \"as is\" and is subject to change or removal at any time.", trainingSet)
html<-grep("style=\"background:", trainingSet)
if(length(footer)>0) { trainingSet<-trainingSet[-footer] }
if(length(html)>0) {trainingSet<-trainingSet[-html]}

training_df<-data_frame(line=1:length(trainingSet), text=trainingSet)
# token is a single word
words<- training_df %>% unnest_tokens(word, text) 
words_wo_sw <- words %>% anti_join(stop_words)

# token is a bigram
bigrams<- training_df %>% unnest_tokens(bigram, text, token="ngrams", n=2) %>% filter(bigram != "NA NA")
bigrams_separated<- bigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated_wo_sw<-bigrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word)
bigrams_wo_sw <- bigrams_separated_wo_sw %>% unite(bigram, word1, word2, sep = " ") %>% filter(bigram != "NA NA")

# token is a trigram
#trigrams<- training_df %>% unnest_tokens(trigram, text, token="ngrams", n=3) %>% filter(trigram != "NA NA NA")
#trigrams_separated<- trigrams %>% separate(trigram, c("word1", "word2", "word3"), sep = " ")
#trigrams_separated_wo_sw<-trigrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word) %>% filter(!word3 %in% stop_words$word)
#trigrams_wo_sw <- trigrams_separated_wo_sw %>% unite(trigram, word1, word2, word3, sep = " ") %>% filter(trigram != "NA NA NA")

# token is a 4-gram
#fourgrams<- training_df %>% unnest_tokens(fourgram, text, token="ngrams", n=4)  %>% filter(fourgram != "NA NA NA NA")
#fourgrams_separated<- fourgrams %>% separate(fourgram, c("word1", "word2", "word3", "word4"), sep = " ")
#fourgrams_separated_wo_sw<-fourgrams_separated %>% filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word) %>% filter(!word3 %in% stop_words$word)%>% filter(!word4 %in% stop_words$word)
#fourgrams_wo_sw <- fourgrams_separated_wo_sw %>% unite(fourgram, word1, word2, word3, word4, sep = " ") %>% filter(fourgram != "NA NA NA NA")
```

The only thing we haven't done here is stemming.

Quiz-Questions:

```{r}
# longest line in documents
#maxLengthBlog<-max(sapply(textLines, nchar))
#maxLengthNews<-max(sapply(textLinesNews, nchar))
#maxLengthTwitter<-max(sapply(textLinesTwitter, nchar))
# love vs. hate
#twitterLove<-grep("love", textLinesTwitter, ignore.case=FALSE)
#twitterHate<-grep("hate", textLinesTwitter, ignore.case = FALSE)
#length(twitterLove)/length(twitterHate)
#grep("biostats", textLinesTwitter, ignore.case = FALSE) #biostats
```

## Exploratory data analysis
The goal of the explorary data analysis is to get some feeling for the data, which means, e.g. finding patterns and understanding data properties. Also the exploratory analysis can suggest a modelling strategy for the next step of the data analysis.

Explorary analysis is mainly done by generating many graphs, quick and dirty. While generating them, there are some principles to follow:

- principle 1: show comparisons: evidence for an hypothesis is always relative to a competing hypothesis
- principle 2: show causality, mechanism, explanation, systematic structure
- principle 3: show multivariate data, as the real world is multivariate
- principle 4: integration of evidence
- principle 5: describe and document the evidence with appropriate labels, sources, etc: a data graphic should tell a complete story that is credible
- principle 6: content is king

Clustering and dimension reduction can be part of the exploratory data analysis.

### stopwords vs. no stopwords
```{r}
c_words<-count(words, word, sort=TRUE)
c_words_wo_sw<-count(words_wo_sw, word, sort=TRUE)
nrow(c_words_wo_sw)/nrow(c_words)
words1<-head(c_words, n=40) %>% mutate(word = reorder(word, n)) %>% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip()
words2<-head(c_words_wo_sw, n=40) %>% mutate(word = reorder(word, n)) %>% ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip()
cowplot::plot_grid(words1,words2)

c_bigrams<-count(bigrams, bigram, sort=TRUE)
c_bigrams_wo_sw<-count(bigrams_wo_sw, bigram, sort=TRUE)
nrow(c_bigrams_wo_sw)/nrow(c_bigrams)
br1<-head(c_bigrams, n=40) %>% mutate(bigram = reorder(bigram, n)) %>% ggplot(aes(bigram, n)) + geom_col() + xlab(NULL) + coord_flip()
bg2<-head(c_bigrams_wo_sw, n=40) %>% mutate(bigram = reorder(bigram, n)) %>% ggplot(aes(bigram, n)) + geom_col() + xlab(NULL) + coord_flip()
cowplot::plot_grid(br1,bg2)

#c_trigrams<-count(trigrams, trigram, sort=TRUE)
#c_trigrams_wo_sw<-count(trigrams_wo_sw, trigram, sort=TRUE)
#nrow(c_trigrams_wo_sw)/nrow(c_trigrams)
#tg1<-head(c_trigrams, n=40) %>% mutate(trigram = reorder(trigram, n)) %>% ggplot(aes(trigram, n)) + geom_col() + xlab(NULL) + coord_flip()
#tg2<-head(c_trigrams_wo_sw, n=40) %>% mutate(trigram = reorder(trigram, n)) %>% ggplot(aes(trigram, n)) + geom_col() + xlab(NULL) + coord_flip()
#cowplot::plot_grid(p1,p2)

#c_fourgrams<-count(fourgrams, fourgram, sort=TRUE)
#c_fourgrams_wo_sw<-count(fourgrams_wo_sw, fourgram, sort=TRUE)
#nrow(c_fourgrams_wo_sw)/nrow(c_fourgrams)
#fg1<-head(c_fourgrams, n=40) %>% mutate(fourgram = reorder(fourgram, n)) %>% ggplot(aes(fourgram, n)) + geom_col() + xlab(NULL) + coord_flip()
#fg2<-head(c_fourgrams_wo_sw, n=40) %>% mutate(fourgram = reorder(fourgram, n)) %>% ggplot(aes(fourgram, n)) + geom_col() + xlab(NULL) + coord_flip()
#cowplot::plot_grid(p1,p2)

```

One could see in all cases of token length that without removing stop words all tokens with the highest counts consist completely or nearly of stop words. But one could type a stop word and we need a prediction for this also. So, for now, we will work with both sets.
As one might expect, the percentage of n-grams without any stopwords compared to n-grams with and without stopwords get lower and lower the longer the tokens get. Also the longer the tokens get the less ofter they appear (also easy and logical).

### term graphs



### Tasks to accomplish

-  understand the distribution of words
-  understand relationship between words
-  unterstand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data

### Questions to consider

1.  Some words are more frequent than others - what are the distributions of word frequencies?  
The frequency of a word, and for this it's probability of occurence, is inversely proportional to its rank in the frequency sorted dictionary. This rule is known as Zipf's law.  

The set of all possible words with rank $n\in \{ 1..N \}$ is finite and for this we can state (please refer to wikipedia):
$p(n) = 1/H_N * 1/n$, where $H_N = \sum_{n\in \{1...N\} } 1/n$, which is about $p(n)\approx 1/(n * ln(1.78 * N))$.
Look at this:
```{r}
rank<-1:nrow(c_words)
c_words<-cbind(c_words, rank)
c_words<-mutate(c_words, inv_rank = 1/rank, zipf=1/(rank*log(1.78*nrow(c_words))), sample = n/sum(c_words$n))
head(c_words)
# we are happy with this appoximation!
first_100<-head(c_words, n=100)
plot(first_100$zipf~first_100$rank, type = "o")
lines(first_100$sample, type="o", col="red")
```
One can easily proof if a data set conforms to Zipf's law by plotting rank versus frequency on a log-log-scale. If the graph is linear the rule holds:

```{r}
plot(log(c_words$n)~log(c_words$rank))
```

mean of the Fitz distro with $s=1$:
```{r}
mean<-nrow(c_words)/sum(c_words$inv_rank) #? not sure
```

Please observe that the parameter $s$ of the Fitz distribution is $1$ in our case.
2.  What are the frequencies of 2-grams and 3-grams in the dataset?
3.  How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?  
We are looking for the rank for which the sum of probablities from 1 to this rank sum up to 0.5.
```{r}
sum(filter(c_words, rank<=114)$sample)
```
4.  How do you evaluate how many of the words come from foreign languages?
5.  Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

## Statistical prediction/clustering

To build our first model we first notice that when calculation the the probability of an n-gram, we can use the chain rule:
Let $w$ be an n-gram of length $k$, so $w=(w_1 ... w_k)$, then:
$P(w)=P((w_1... w_k))=P(w_1)P(w_1|w_2)...P(w_k|w_1 ... w_{k-1})$.

But it's is hard to determine the probability of the last word given all the previous words. That's is why we need the markov assumption which states, that we can estimate this probability by only using the last few words, or even only the last word. With this assumption we get:
$P(w)=P((w_1... w_k))=P(w_1)P(w_1|w_2)...P(w_k| w_{k-1})$.

$P(w)=P((w_1... w_k))=P(w_1|start)P(w_1|w_2)...P(w_k| w_{k-1})*P(w_k|end)$.

On base of this we can build n-gram models, also called markov models. The most easiest model is a unigram model. In this model, each word is a unite. This doesn't help us, so we start by building a bigram model. In this model we predict conditioned on the previous word. 
n-gram models in general, even 5-gram oder bigger models, are not sufficient to estimate natural language, because there exist long distance relationsships, but for our scenario they are!

## Appendix

### Natural language processing with tm
The main structure for managing documents with the package tm is a so called corpus, representing  a  collection  of  text documents. There exist two types of corpora, volatile and permanent. Volatile VCorpora are held fully in memory, Permanent PCorpora are physically stored, e.g. on the local file system.

A VCorpus can be constructed via the Constructor:
```{r, eval=FALSE}
VCorpus(x, readerControl)
```

Here, $x$ is a $Source$-Object. There exist difference Source types, e.g. $DirSource$, $DataframeSource$ and $VectorSource$.
The second argument $readerControl$ of the corpus constructor has to be a list with the named components $reader$ and $language$. The first component $reader$ constructs a text document from elements delivered by a source. Each $Source$ has a default $reader$, which can be overwritten.
Finally, the second component $language$ sets the texts' language (preferably using ISO 639-2 codes).

E.g. one can construct a volatile corpus reading txt-files from a directory with the following command:
```{r, eval=FALSE}
source<-DirSource("directory", encoding="UTF-8")
VCorpus(source, readerControl = list(language = "lat")))
```

Here we use the default $reader$ of the $DirSource$.

If you have manipulated some text and you want to save it on the local file system you can use
```{r, eval=FALSE}
writeCorpus(yourCorpus)
```

To get the text inside of a document in a Corpus one can use
```{r, eval=FALSE}
yourCorpus[[id]]
```

You can use methods for standard texts in R on Corpora by applying the $tm_map(yourcorpus, function)$ function.

### R packages for text mining



tm, RTextTools, topicmodels

## References
[1] https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
[2] https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf
[3] https://www.tidytextmining.com/
[4] https://www.rdocumentation.org/packages/VGAM/versions/1.0-6/topics/zipf
