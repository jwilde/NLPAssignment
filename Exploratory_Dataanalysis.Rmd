---
title: "Exploratory Data Analysis of the SwiftKey Dataset"
author: "J. Wilde"
date: "26 February 2019"
output:
  html_document: default
  pdf_document: default
subtitle: Milestone Report
references:
- URL: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
  id: swiftkeydata
  title: SwiftKey DataSet
- id: silgerobinson2019
  title: Text Mining with R
  author:
  - family: Silge
    given: Julia
  - family: Robinson
    given: David
  container-title: A Tidy Approach
  URL: 'https://www.tidytextmining.com/'
  publisher: O'Reilly
  issued:
    year: 2019
    month: 2
- URL: https://en.wikipedia.org/wiki/Zipf%27s_law
  id: wikizipf
  title: Wikipedia article about Zipf's law  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Report Overview

The goal of the capstone project is to built a predictive model for natural language processing. Based on an input in form of a couple of words that were typed already the next word(s) shall be predicted. 

For this Swiftkey provided a dataset of natural language including text from news, twitter posts and blog posts in different languages [see @swiftkeydata].

This report shows the results of the exploratory analysis done based on the english data.
Also an outlook is provided for creating a prediction algorithm and an app.

### Provided Data

For the english language we got three documents for each of the three sources.

```{r read_data, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
textLinesBlog<-readLines("en_US.blogs.txt", encoding="UTF-8", warn=FALSE)
textLinesTwitter<-readLines("en_US.twitter.txt", encoding="UTF-8", warn=FALSE)
textLinesNews<-readLines("en_US.news.txt", encoding="UTF-8", warn=FALSE)
```

To get an idea of the size of these documents we count the number of characters, words and lines in each document:
```{r basic_stats, message=FALSE, echo=FALSE, warning=FALSE, cache=TRUE}
library(ngram)
library(dplyr)
library(knitr)
library(kableExtra)
#blog, twitter, news
lines<-c(length(textLinesBlog), length(textLinesTwitter), length(textLinesNews))
words<-c(wordcount(textLinesBlog, sep = " ", count.function = sum), wordcount(textLinesTwitter, sep = " ", count.function = sum), wordcount(textLinesNews, sep = " ", count.function = sum))
characters<-c(sum(nchar(textLinesBlog)),  sum(nchar(textLinesTwitter)),  sum(nchar(textLinesNews)))
basicFigures<-as.data.frame(rbind(lines, words, characters)) %>% rename(blog=V1, twitter=V2, news=V3)
kable(basicFigures) %>% kable_styling(full_width = F)
```

Here are extracts of the texts given:
```{r extracts, echo=FALSE, cache=TRUE}
textLinesBlog[102]
textLinesTwitter[202]
textLinesNews[302]
```

### Basic data cleaning and tokenization

The provided data is not tidy. According to [@silgerobinson2019] the tidy text format is a table with "one-token-by-row". Therefore we have to so called tokezine our data. A token is a meaningful unit of text, such as a word or a sentence. 

```{r sample, echo=FALSE, cache=TRUE}
set.seed(1111)
blogSample<-sample(textLinesBlog, length(textLinesBlog)*0.1)
newsSample<-sample(textLinesNews, length(textLinesNews)*0.1)
twitterSample<-sample(textLinesTwitter, length(textLinesTwitter)*0.1)
```

First we split the data at non-alphabetic characters and remove these characters, too. The result is:

- If a sentence includes no numbers or commas we get the whole sentence 
- If a sentence includes numbers the sentence gets split where the numbers are  
- If a sentence includes commas the sentence gets split into his sentence parts  

Also we do further transformations like whitespace removal and lower case conversion.

The main goal of this first step is to get a clean data set, but without loosing too much information about the text already. The result looks something like this:

```{r clean, warning=FALSE, message=FALSE, echo=FALSE, cache=TRUE}
library(tidytext)
library(dplyr)
library(tidyverse)
blogSampleText<-paste(blogSample, " ")
cleanBlog<-strsplit(blogSampleText, "([^[:alpha:]][^[:alpha:]])|^[^[:alpha:]]|[^[:alpha:]]$") %>% unlist() %>% iconv( "UTF-8", "ASCII", sub="") %>% str_squish() %>% tolower()
cleanBlog<-cleanBlog[cleanBlog != ""]
newsSampleText<-paste(newsSample, " ")
cleanNews<-strsplit(newsSampleText, "([^[:alpha:]][^[:alpha:]])|^[^[:alpha:]]|[^[:alpha:]]$") %>% unlist() %>% iconv( "UTF-8", "ASCII", sub="")%>% str_squish() %>% tolower()
cleanNews<-cleanNews[cleanNews != ""]
twitterSampleText<-paste(twitterSample, " ")
cleanTwitter<-strsplit(twitterSampleText, "([^[:alpha:]][^[:alpha:]])|^[^[:alpha:]]|[^[:alpha:]]$") %>% unlist() %>% iconv( "UTF-8", "ASCII", sub="") %>% str_squish() %>% tolower()
cleanTwitter<-cleanTwitter[cleanTwitter != ""]

sampleData<-data_frame(line=1:length(cleanBlog), source='blog', text=cleanBlog)
sampleData<-rbind(sampleData, data_frame(line=1:length(cleanNews), source='news', text=cleanNews))
sampleData<-rbind(sampleData, data_frame(line=1:length(cleanTwitter), source='twitter', text=cleanTwitter))
kable(head(sampleData, n=3)) %>% kable_styling(full_width = F)
```

We tokenize these text parts into so called n-grams of size one to four. For example a 1-gram or unigram is a single word, a 2-gram or bigram are two words, etc.
Also we mark each n-gram if it includes stopwords or not. 

```{r tokenize, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
stopWords<-as.data.frame(unique(stop_words$word))%>%rename(word='unique(stop_words$word)')%>%mutate(isStopWord=TRUE)

#unigrams
words<- sampleData %>% unnest_tokens(word, text) %>% left_join(stopWords, by="word")%>%mutate(isStopWord = if_else(is.na(isStopWord), FALSE, isStopWord))
kable(head(words, n=3))%>% kable_styling(full_width = F)
```

For the other n-grams this looks similar.

```{r tokenize_bi, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
library(knitr)
library(kableExtra)
#bigrams
bigrams<- sampleData %>% unnest_tokens(bigram, text, token="ngrams", n=2) %>% filter(!is.na(bigram))%>% separate(bigram, c("word1", "word2"), sep = " ", remove=FALSE)
bigrams<-left_join(bigrams, stopWords, by = c("word1" = "word"))%>%left_join(stopWords, by = c("word2" = "word"))%>% mutate(isStopWord.x = if_else(is.na(isStopWord.x), FALSE, isStopWord.x), isStopWord.y = if_else(is.na(isStopWord.y), FALSE, isStopWord.y))%>%mutate(includesStopWord=(isStopWord.x | isStopWord.y)) %>%select(c("line", "source", "bigram", "word1", "word2", "includesStopWord"))
```

```{r tokenize_tri, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
#trigrams
trigrams<- sampleData %>% unnest_tokens(trigram, text, token="ngrams", n=3) %>% filter(!is.na(trigram))%>% separate(trigram, c("word1", "word2", "word3"), sep = " ", remove=FALSE)
trigrams<-left_join(trigrams, stopWords, by = c("word1" = "word")) %>% left_join(stopWords, by = c("word2" = "word")) %>% left_join(stopWords, by = c("word3" = "word")) %>% mutate(isStopWord.x = if_else(is.na(isStopWord.x), FALSE, isStopWord.x), isStopWord.y = if_else(is.na(isStopWord.y), FALSE, isStopWord.y), isStopWord = if_else(is.na(isStopWord), FALSE, isStopWord))%>%mutate(includesStopWord=(isStopWord.x | isStopWord.y |isStopWord)) %>%select(c("line", "source", "trigram", "word1", "word2", "word3", "includesStopWord"))
```

```{r tokenize_four, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE}
#fourgrams
fourgrams<- sampleData %>% unnest_tokens(fourgram, text, token="ngrams", n=4) %>% filter(!is.na(fourgram))%>% separate(fourgram, c("word1", "word2", "word3", "word4"), sep = " ", remove=FALSE)
fourgrams<-left_join(fourgrams, stopWords, by = c("word1" = "word")) %>% left_join(stopWords, by = c("word2" = "word")) %>% left_join(stopWords, by = c("word3" = "word")) %>% left_join(stopWords, by = c("word4" = "word")) %>% mutate(isStopWord.x = if_else(is.na(isStopWord.x), FALSE, isStopWord.x), isStopWord.y = if_else(is.na(isStopWord.y), FALSE, isStopWord.y), isStopWord.x.x = if_else(is.na(isStopWord.x.x), FALSE, isStopWord.x.x), isStopWord.y.y = if_else(is.na(isStopWord.y.y), FALSE, isStopWord.y.y))%>%mutate(includesStopWord=(isStopWord.x | isStopWord.y |isStopWord.x.x | isStopWord.y.y)) %>%select(c("line", "source", "fourgram", "word1", "word2", "word3", "word4", "includesStopWord"))
```

### Exploratory Analysis

To get an impression of the data we have a look at the most common words in the data set.

```{r basic_expl, warning=FALSE, message=FALSE, echo=FALSE}
library(ggplot2)
library(cowplot)
library(wordcloud)
library(maditr)
library(dplyr)
library(tidyverse)
# calculate count of each unigram grouped by source
wordStatistics<-words%>% group_by(source, word, isStopWord)%>%summarize(n=n())%>%dcast(word+isStopWord~source, value.var="n") %>% mutate_all(funs(replace_na(.,0))) %>% mutate( frequency=blog+news+twitter) 

wordcloudBase<-filter(wordStatistics, isStopWord==FALSE)

# print some nice diagrams
wordcloud(words=wordcloudBase$word, freq=wordcloudBase$frequency, min.freq=1, scale = c(4, 0.2), max.words=80, random.order=FALSE,rot.per=0.35, colors=brewer.pal(8, "Set1"))

```

Another way to show this is via a histogram. For this we order the words by frequency and print the first $35$ words.
```{r plot_words, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
wordWoStopwordsPlot<-wordStatistics%>%filter(isStopWord==FALSE)%>% arrange(desc(frequency)) %>% head(n=35) %>% mutate(word=reorder(word, frequency)) %>% ggplot(aes(word, frequency)) + geom_col(fill='steelblue2') + xlab(NULL) + coord_flip() +ggtitle("most frequent words/no stopwords")+theme(plot.title = element_text(size = 12, face = "bold"), axis.text.y = element_text(size=8), text=element_text(size=10), axis.text.x=element_text(size=10))#+labs(x="frequency")
wordPlot<-wordStatistics %>% arrange(desc(frequency)) %>% head(n=35)  %>% mutate(word=reorder(word, frequency)) %>% ggplot(aes(word, frequency)) + geom_col(fill='lightskyblue2') + xlab(NULL)+ coord_flip()+ ggtitle("most frequent words")+theme(plot.title = element_text(size = 12, face = "bold"), axis.text.y = element_text(size=8), text=element_text(size=10), axis.text.x=element_text(size=10)) 
cowplot::plot_grid(wordWoStopwordsPlot,wordPlot)
```

Based on this view we get an idea of the distribution of words. The higher the rank in this frequency ordered table of words, the more the word count tends to $0$.
One can formulate this a bit more detailed by referencing to Zipf's law, which states:  

> The frequency of a word, and for this it's probability of occurence, is inversely proportional to its rank in the frequency sorted dictionary. 

[see @wikizipf]

For further analysis the frequency is normalized as to depict the "sample probability".

```{r prep_stat, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE }
library(dplyr)
rank<-1:nrow(wordStatistics)
sumCount<-sum(wordStatistics$frequency)
wordStatistics<-arrange(wordStatistics, desc(frequency)) %>% cbind(rank) %>% mutate(relativeFrequency=frequency/sumCount)
```

If we plot the relative count on a log-log-scale, the graph suggests a general linear model.

```{r lm_smoother, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
ggplot(data=head(wordStatistics, n=500), aes(x=log(rank), y=log(relativeFrequency)))+geom_line(aes(color="sample"), size=1)+geom_point(aes(color="sample")) + xlab("log(rank)")+ylab("log(relative frequency)")+ geom_smooth(method="lm", size=1, aes(color="smoother")) + scale_color_manual(name = "legend", values = c("steelblue2","red"))
```

So we are going to try to estimate the distribution using the following model:

$$ y = \alpha r^{\beta} $$where $r$ is the rank of the word and $y$ is the relative frequency. Doing a log-transformation we get:
$$ \log(y) = \log(\alpha) + \beta\log(r) $$. 

We can estimate the parameters:
```{r lm_est, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
linearModel<-lm(log(head(wordStatistics, n=500)$rank)~log(head(wordStatistics, n=500)$relativeFrequency))
intercept = linearModel$coefficients[1]
factor = linearModel$coefficients[2]
wordStatistics<-mutate(wordStatistics, calculatedRelativeFrequency=(exp(1)^intercept)*(rank^(factor)))
summary(linearModel)
```

The p-value is significantly small, R square is significantly high. So this seems to be a good fit to the given data.

```{r plot_resid, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE} 
ggplot(data=head(wordStatistics, n=500), aes(x=log(rank), y=resid(linearModel)))+geom_point(color="steelblue2") + geom_hline(yintercept=0, color="red", size=0.5)+ xlab("log(rank)")+ylab("residual")
```

There are stronger differences for lower ranks, but in general there is no obvious pattern in the residuals plot. If we plot the calcucated relative frequencies and the sample relative frequencies we can see, that the model fits well. 

```{r plot_model, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(data=head(wordStatistics, n=500), aes(x=rank))+geom_line(aes(y=relativeFrequency,color="sample"), size=1)+geom_point(aes(y=relativeFrequency),color="steelblue2") + geom_line(aes(y=calculatedRelativeFrequency,color="log-log-model"), size=1)+ scale_color_manual(name = "legend", values = c("red","steelblue2"))+ xlab("log(rank)")+ylab("log(relative frequency)")
```

Only the first ranks have a stronger difference. Because these are the ranks with the highest frequency this is a problem if we want to estimate how many unique words we need in a frequency sorted dictionary to cover 50% of all word instances in the sample text.

If we calculate this using the log-log-linear model, we get:
```{r calc_model_coverage, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
find_rank_to_coverage<-function(log_log_model, coverage){
  #coverage in decimal
  freq_sum=0
  rank=1
  repeat{
    freq_sum=freq_sum+(exp(1)^log_log_model$coefficients[1])*(rank^(log_log_model$coefficients[2]))
    rank=rank+1
    if(freq_sum >= coverage){
      break
    }
  }
  rank
}

find_rank_to_coverage(linearModel, 0.5)
``` 

But if we calculate it using the sample frequencies we get:
```{r calc_coverage, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
find_rank_to_coverage_sample<-function(sample, coverage){
  #coverage in decimal
  freq_sum=0
  cur_rank=1
  repeat{
    freq_sum=freq_sum+subset(sample, rank==cur_rank)$relativeFrequency
    cur_rank=cur_rank+1
    if(freq_sum >= coverage){
      break
    }
  }
  cur_rank
}

find_rank_to_coverage_sample(wordStatistics, 0.5)
```

For higher percentages, e.g. $90%$, the difference gets even bigger.


One can do similar analyses for higher order n-grams. 
```{r ngram_stats, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
bigramStatistics<-bigrams %>% group_by(source, bigram, word1, word2, includesStopWord) %>%summarize(n=n())%>%dcast(bigram+includesStopWord +word1+word2~source, value.var="n") %>% mutate_all(funs(replace_na(.,0))) %>% mutate( frequency=blog+news+twitter) 
trigramStatistics<-trigrams %>% group_by(source, trigram, word1, word2, word3, includesStopWord) %>%summarize(n=n())%>%dcast(trigram+includesStopWord+word1+word2+word3~source, value.var="n") %>% mutate_all(funs(replace_na(.,0))) %>% mutate( frequency=blog+news+twitter) 

rank<-1:nrow(bigramStatistics)
sum_count<-sum(bigramStatistics$frequency)
bigramStatistics<-arrange(bigramStatistics, desc(frequency)) %>% cbind(rank) %>%mutate(relativeFrequency=frequency/sum_count)

rank<-1:nrow(trigramStatistics)
sum_count<-sum(trigramStatistics$frequency)
trigramStatistics<-arrange(trigramStatistics, desc(frequency)) %>% cbind(rank) %>%mutate(relativeFrequency=frequency/sum_count)
```

Interesting to notice: the longer the n-grams get, the more balanced the distribution is, meaning, the difference between the lower-ranked n-grams the higher-ranked n-grams gets smaller and smaller.

```{r distri, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
ggplot(NULL, aes(x=rank, y=relativeFrequency))+geom_line(data=head(wordStatistics, n=500), aes(color="unigrams"), size=1)+geom_point(data=head(wordStatistics, n=500), color="grey")+geom_line(data=head(trigramStatistics, n=500), aes(color="trigrams"), size=1)+geom_point(data=head(trigramStatistics, n=500),color="steelblue2")+geom_line(data=head(bigramStatistics, n=500), aes(color="bigrams"), size=1)+geom_point(data=head(bigramStatistics, n=500),color="red")+xlab("rank")+ylab("relative frequency")+coord_cartesian(ylim=c(0, 0.01)) +scale_color_manual(name = "legend", values = c("red","steelblue2", "grey"))
```


Until now we did an analysis only based on frequencies of n-grams with fixed n. Now it would be interesting to have a look on the relationship between words. One can think about the following question:
What are the most common words that are part of a bigram (on first or second position)/the most common bigrams that are part of a trigram, etc.?

Here we calculate the most common words in bigrams and compare them to the most common unigrams:
```{r bigrams_and_unigrams, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
library(CGPfunctions)
pos1<-bigramStatistics%>%group_by(word1) %>%summarize(frequency=sum(frequency))
pos2<-bigramStatistics%>%group_by(word2) %>%summarize(frequency=sum(frequency))
wordsInBigrams<-merge(x=pos1, y=pos2,by.x= 'word1', by.y='word2' )%>%mutate(frequency=frequency.x+frequency.y)%>%rename(word=word1)%>%select( c("word", "frequency"))%>%arrange(desc(frequency))
rank<-1:nrow(wordsInBigrams)
wordsInBigrams<-cbind(wordsInBigrams, rank)
comparison<-merge(wordStatistics, wordsInBigrams, by="word")%>%rename(freqeuncyUnigram=frequency.x, frequencyBigram=frequency.y, rankUnigram=rank.x, rankBigram=rank.y)%>%select(word, rankUnigram, rankBigram)%>%melt(id.vars=c("word"), measured.vars=c("rankUnigram", "rankBigram"))
hitlist<-comparison[which(value<=30),]
newggslopegraph(hitlist, variable, value, word, Title="word ranks", SubTitle="comparison of ranks of words as unigrams and as part of bigrams", Caption=NULL, LineColor="steelblue2", LineThickness=1)
``` 

So there is no big difference in ranks. This is also the case if we look at bigrams and trigrams.

```{r trigrams_and_bigrams, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval=FALSE}
library(CGPfunctions)
pos1<-trigramStatistics%>%group_by(word1, word2) %>%summarize(frequency=sum(frequency)) %>% mutate(bigram=str_squish(paste(word1,word2, sep=" ")))
pos2<-trigramStatistics%>%group_by(word2, word3) %>%summarize(frequency=sum(frequency)) %>% mutate(bigram=str_squish(paste(word2,word3, sep=" ")))
bigramsInTrigrams<-merge(x=pos1, y=pos2,by.x= 'bigram', by.y='bigram' )%>%mutate(frequency=frequency.x+frequency.y)%>%select(-c("word2.x", "frequency.x", "word2.y", "frequency.y"))%>%rename(word2=word3)%>%arrange(desc(frequency))
rank<-1:nrow(bigramsInTrigrams)
bigramsInTrigrams<-cbind(bigramsInTrigrams, rank)
comparison<-merge(bigramStatistics, bigramsInTrigrams, by="bigram")%>%rename(freqeuncyBigram=frequency.x, frequencyTrigram=frequency.y, rankBigram=rank.x, rankTrigram=rank.y)%>%select(bigram, rankBigram, rankTrigram)%>%melt(id.vars=c("bigram"), measured.vars=c("rankBigram", "rankTrigram"))
hitlist<-comparison[which(value<=30),]
newggslopegraph(hitlist, variable, value, bigram, Title="bigram ranks", SubTitle="comparison of ranks of bigrams and as part of trigrams", Caption=NULL, LineColor="steelblue2", LineThickness=1)
``` 

### Outlook for the prediction algorithm and the shiny app

Based on this analysis the next step is to build a prediction algorithm. This algorithm shall predict for a given sequence of words as input the most probable words that come next. 

There are many features about text that could be used for this prediction. The most obvious feature is to use the word frequency, because of course in real language some words are used more often than others and this is not only a feature of our training data set.
But one can think also, for example, about syntactical approaches, where the grammar matters. For this POS tagging is common. 
Because it is hard to model all of the features of a text neural nets are widely used nowadays.

For now we concentrate on the frequency based approach. Based on frequencies of n-grams one can predict the next word based on the last words. Of course we loose history when we use this approach, and usually the history matters, because of the text topic, syntax, etc. But n-gram-models are not that ressource-consuming compared to other approaches and often other approaches only result in slight improvements.

Based on the prediction algorith we will also provide a frontend in form of a shiny app where one can type a text and gets suggestions for the next words one wants to type.

# References
